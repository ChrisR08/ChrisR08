{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ChrisR08/ChrisR08/blob/main/Genersis_Query_Google.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RSH8tnGVFWAE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f789d25f-9276-4c67-a580-b4034c11f6af"
      },
      "source": [
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "import gspread\n",
        "import pandas as pd\n",
        "import time, random\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from google.colab import files\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sV5ju8ucPQpM",
        "cellView": "form"
      },
      "source": [
        "#@title Methods\n",
        "from bs4 import BeautifulSoup\n",
        "from requests import get\n",
        "\n",
        "def fetch_results(search_term, number_results, language_code):\n",
        "  usr_agent = {\n",
        "      'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36'\n",
        "  }\n",
        "  escaped_search_term = search_term.replace(' ', '+')\n",
        "  google_url = 'https://www.google.com/search?q={}&num={}&hl={}'.format(escaped_search_term, number_results+1, language_code)\n",
        "  response = get(google_url, headers=usr_agent)\n",
        "  response.raise_for_status()\n",
        "  return response.text\n",
        "\n",
        "def parse_results(raw_html):\n",
        "  soup = BeautifulSoup(raw_html, 'html.parser')\n",
        "  result_block = soup.find_all('div', attrs={'class': 'g'})\n",
        "  for result in result_block:\n",
        "    link = result.find('a', href=True)\n",
        "    title = result.find('h3')\n",
        "    if link and title:\n",
        "      yield (link['href'], title.text)\n",
        "\n",
        "def pullFirst10GoogleResults(query):\n",
        "    my_results_list = []\n",
        "    html    = fetch_results(query, 10, \"en\")\n",
        "    res_lst = list(parse_results(html))\n",
        "    return res_lst\n",
        "\n",
        "def pullGoogleSerps(query_lst):\n",
        "  serp_lst = []\n",
        "  for q in query_lst:\n",
        "    serps = pullFirst10GoogleResults(q)\n",
        "    for serp in serps:\n",
        "      serp_lst.append(serp)\n",
        "    time.sleep(random.randint(2,5))\n",
        "  return serp_lst\n",
        "\n",
        "def cleanText(text):\n",
        "    words = text.split()\n",
        "    # convert to lower case\n",
        "    words = [word.lower() for word in words]\n",
        "    # remove all tokens that are not alphabetic\n",
        "    words = [word for word in words if word.isalpha()]\n",
        "    # The Porter Stemmer had poor results comparing ot the Lemmatizer, both for verbs and nouns/adjectives.\n",
        "    words = [WordNetLemmatizer().lemmatize(x, 'v') for x in words]\n",
        "    return words\n",
        "\n",
        "def matchesIntention( kw_lst, query_lst):\n",
        "    intent_kw_set = set(kw_lst)\n",
        "    query_kw_set  = set(query_lst)\n",
        "    return bool(intent_kw_set.intersection(query_kw_set))\n",
        "\n",
        "def findSearchIntent(query):\n",
        "    ### Transactional Phase: The user is actively looking to make a purchase\n",
        "    transactional_kws = ['buy','sale','let','rent','get','coupon','discount','deal','shipping','ship', 'purchase', 'acquire', 'transaction']\n",
        "    ### Research Phase: Decided that they are purchasing something and trying to find more information about it\n",
        "    research_kws      = ['review','best', 'good', 'bad', 'wordst', 'awesome', 'amazing', 'top','cheap','affordable','comparison', 'vs', \"research\" ]\n",
        "    ### Informational Phase: The users are trying to build things themselves\n",
        "    informational_kws = ['intro','diy','tutorial','guide','explain','demo','way to','ways to','i need to', 'tips','tricks',\"information\",\"know\",\"manual\", \"explaination\"]\n",
        "    ### Question Phase: The users is in the beginning of the buying cycle and navigates by asking questions\n",
        "    question_kws      = ['how','what','who','when','whose','why','where']\n",
        "    ### Sticky Phase: Unlikely to convery now or every\n",
        "    sticky_kws        = ['free', 'torrent', 'download', 'hack']\n",
        "    ### Generic/Brand: Don't know how to distinguish Brand Names presently.\n",
        "\n",
        "    cleanQuery = cleanText(query)\n",
        "\n",
        "    returnFlag   = False\n",
        "    returnString = ''\n",
        "\n",
        "    if matchesIntention(transactional_kws, cleanQuery):\n",
        "        returnString += \"Transactional\"\n",
        "        returnFlag = True\n",
        "    if matchesIntention(research_kws, cleanQuery):\n",
        "        if returnFlag: returnString += \" & \"\n",
        "        returnString += \"Research\"\n",
        "        returnFlag = True\n",
        "    if matchesIntention(informational_kws, cleanQuery):\n",
        "        if returnFlag: returnString += \" & \"\n",
        "        returnString += \"Informational\"\n",
        "        returnFlag = True\n",
        "    if matchesIntention(question_kws, cleanQuery):\n",
        "        if returnFlag: returnString += \" & \"\n",
        "        returnString += \"Question\"\n",
        "        returnFlag = True\n",
        "    if matchesIntention(sticky_kws, cleanQuery):\n",
        "        if returnFlag: returnString += \" & \"\n",
        "        returnString += \"Sticky\"\n",
        "        returnFlag = True\n",
        "    if returnFlag == False: returnString = \"Generic/Brand\"\n",
        "\n",
        "    return returnString\n",
        "\n",
        "def prepareFinalDf(serp_lst):\n",
        "  priority_lst = []\n",
        "  basic_lst    = []\n",
        "  for serp in serp_lst:\n",
        "    for prior_kw in Priority_Keywords:\n",
        "      if prior_kw.lower() in serp[1].lower():\n",
        "        priority_lst.append(serp)\n",
        "      else:\n",
        "        basic_lst.append(serp)\n",
        "\n",
        "  priority_df = pd.DataFrame(priority_lst, columns=[\"URL\", \"Title\"]).drop_duplicates()\n",
        "  priority_df[\"Priority\"] = 1\n",
        "  basic_df = pd.DataFrame(basic_lst, columns=[\"URL\", \"Title\"]).drop_duplicates()\n",
        "  basic_df[\"Priority\"] = 2\n",
        "  tot_df = pd.concat([priority_df, basic_df])\n",
        "  tot_df[\"Intent\"] = tot_df.apply(lambda x: findSearchIntent(x[\"Title\"]), axis=1)\n",
        "  tot_df.sort_values([\"Priority\", \"Intent\",\"URL\"], ascending=[True, False, True], inplace=True)\n",
        "  return tot_df\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wfcLll-VRz8f"
      },
      "source": [
        "queries_df  = pd.read_csv(\"/content/gdrive/MyDrive/queries.csv\")\n",
        "queries_lst = queries_df[\"query\"].tolist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sDT8o80uQO7_"
      },
      "source": [
        "serp_lst    = pullGoogleSerps(queries_lst)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "m_bWRNuBvX7I"
      },
      "source": [
        "#@markdown ---\n",
        "#@markdown Please select the keywords you wish to target and look for the titles of your target URLs.<br>\n",
        "Priority_Keywords = 'dog, pet, cat'  #@param {type: \"string\"}\n",
        "Priority_Keywords = [x.strip() for x in Priority_Keywords.split(\",\")]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "La6aSDA_6oZI"
      },
      "source": [
        "tot_df = prepareFinalDf(serp_lst)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "78VtZe3M6wF9",
        "outputId": "c909a601-ec78-4a91-c093-e1a2fe368069"
      },
      "source": [
        "tot_df[[\"Priority\", \"Intent\", \"Title\", \"URL\"]].to_csv('target_urls.csv', index=False)\n",
        "files.download('target_urls.csv')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_2e9d4a74-917a-48d5-a2b4-ecd66dd587cd\", \"target_urls.csv\", 67011)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}